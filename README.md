# Train - Self-Attention

In this project, I have implemented a self-attention mechanism from scratch. The model has been trained on Shakespeare's text using a character-level tokenizer. The self-attention mechanism allows the model to weigh the importance of different characters in the input sequence, enabling it to capture long-range dependencies and improve the quality of text generation.

## Implementation Details

- **Self-Attention Mechanism**: The core component of the model, responsible for dynamically focusing on relevant parts of the input sequence.
- **Character-Level Tokenizer**: Tokenizes the input text at the character level, allowing the model to work with a fine-grained representation of the text.
- **Training Data**: The complete works of Shakespeare, providing a rich and diverse dataset for training the model.



## Conclusion

This project was heavly inspired by https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1829s and was only done to understand better how transformers work.